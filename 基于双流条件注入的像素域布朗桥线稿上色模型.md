# åŸºäºåŒæµæ¡ä»¶æ³¨å…¥çš„åƒç´ åŸŸå¸ƒæœ—æ¡¥çº¿ç¨¿ä¸Šè‰²æ¨¡å‹

## å®Œæ•´æŠ€æœ¯æ–¹æ¡ˆæ–‡æ¡£

---

## ä¸€ã€ç ”ç©¶èƒŒæ™¯ä¸ä»»åŠ¡å®šä¹‰

### 1.1 ä»»åŠ¡å®šä¹‰

**è¾“å…¥**ï¼š
- çº¿ç¨¿å›¾åƒ $L \in \mathbb{R}^{1 \times H \times W}$ï¼ˆç°åº¦ï¼ŒåŒ…å«ç»“æ„è½®å»“ä¿¡æ¯ï¼‰
- å‚è€ƒå›¾åƒ $R \in \mathbb{R}^{3 \times H \times W}$ï¼ˆå½©è‰²ï¼Œæä¾›é¢œè‰²é£æ ¼å‚è€ƒï¼‰

**è¾“å‡º**ï¼š
- ä¸Šè‰²å›¾åƒ $\hat{C} \in \mathbb{R}^{3 \times H \times W}$ï¼ˆå½©è‰²ï¼Œä¿æŒçº¿ç¨¿ç»“æ„ï¼Œç»§æ‰¿å‚è€ƒå›¾é¢œè‰²é£æ ¼ï¼‰

### 1.2 æ ¸å¿ƒæŒ‘æˆ˜

| æŒ‘æˆ˜ | å…·ä½“è¡¨ç° | æŠ€æœ¯éš¾ç‚¹ |
|:---|:---|:---|
| **è¯­ä¹‰å¯¹é½** | å‚è€ƒå›¾ä¸çº¿ç¨¿å§¿æ€/è§†è§’ä¸åŒï¼Œé¢œè‰²æ˜ å°„æ˜“é”™ä½ | éœ€å»ºç«‹è·¨å›¾åƒçš„è¯­ä¹‰å¯¹åº”å…³ç³» |
| **ç»“æ„ä¿æŒ** | ä¸Šè‰²åçº¿æ¡æ–­è£‚ã€è¾¹ç¼˜æ¨¡ç³Š | æ‰©æ•£è¿‡ç¨‹ä¸­é«˜é¢‘ä¿¡æ¯æ˜“ä¸¢å¤± |
| **è¾¹ç¼˜é”åˆ©** | ç”Ÿæˆå›¾åƒè¾¹ç¼˜ä¸æ¸…æ™°ï¼Œå‡ºç°"æº¢è‰²"ç°è±¡ | æ‰©æ•£æ¨¡å‹å¤©ç„¶å€¾å‘äºå¹³æ»‘è¾“å‡º |

### 1.3 æ–¹æ¡ˆé€‰å‹ç†ç”±

| æ–¹æ¡ˆ | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‰æ‹© |
|:---|:---|:---|:---:|
| GAN (Pix2Pix) | å¿«é€Ÿæ¨ç† | æ¨¡å¼åå¡Œï¼Œç»†èŠ‚ä¸è¶³ | âŒ |
| Latent BBDM | è®¡ç®—é«˜æ•ˆ | VQGANå‹ç¼©ä¸¢å¤±ç»“æ„ä¿¡æ¯ | âŒ |
| **Pixel BBDM** | ç»“æ„ä¿æŒå¥½ï¼Œè¾¹ç¼˜æ¸…æ™° | è®¡ç®—é‡ç¨å¤§ | âœ… |

**é€‰æ‹©åƒç´ åŸŸBBDMçš„æ ¸å¿ƒç†ç”±**ï¼š
1. **å¸ƒæœ—æ¡¥ç‰¹æ€§**ï¼šå¤©ç„¶æ”¯æŒ"èµ·ç‚¹â†’ç»ˆç‚¹"çš„æ¡ä»¶ç”Ÿæˆï¼Œçº¿ç¨¿ç°åº¦å›¾ä½œä¸ºèµ·ç‚¹çº¦æŸ
2. **æ— å‹ç¼©æŸå¤±**ï¼šç›´æ¥åœ¨åƒç´ ç©ºé—´æ“ä½œï¼Œè¾¹ç¼˜ã€ç»†èŠ‚ä¿¡æ¯å®Œæ•´ä¿ç•™
3. **ç»“æ„å¤©ç„¶å¯¹é½**ï¼šçº¿ç¨¿ç›´æ¥å‚ä¸æ‰©æ•£è¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–çš„ç»“æ„ä¿æŒæ¨¡å—

---

## äºŒã€æ ¸å¿ƒæ¶æ„è®¾è®¡

### 2.1 ç³»ç»Ÿæ€»ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              ç³»ç»Ÿè¾“å…¥                                        â”‚
â”‚                     çº¿ç¨¿ L [1,H,W]      å‚è€ƒå›¾ R [3,H,W]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                                       â”‚
                  â”‚                                       â–¼
                  â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚                        â”‚     å‚è€ƒå›¾ç¼–ç å™¨ (è½»é‡CNN)        â”‚
                  â”‚                        â”‚     4å±‚ä¸‹é‡‡æ ·ï¼Œè¾“å‡ºç‰¹å¾åºåˆ—       â”‚
                  â”‚                        â”‚     context: [B, C, h, w]        â”‚
                  â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                                        â”‚
                  â–¼                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              U-Net å»å™ªç½‘ç»œ                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚    è¾“å…¥: concat(x_t, L) = [3+1=4, H, W]    â† çº¿ç¨¿Concatï¼Œç»“æ„æ— æŸ     â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚    â”‚ Encoder â”‚ â”€â”€â–º â”‚ Middle Block    â”‚ â”€â”€â–º â”‚ Decoder â”‚ â”€â”€â–º â”‚Output â”‚ â”‚  â”‚
â”‚  â”‚    â”‚         â”‚     â”‚ + Cross-Attn    â”‚     â”‚         â”‚     â”‚[3,H,W]â”‚ â”‚  â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                             â–²                                         â”‚  â”‚
â”‚  â”‚                             â”‚ context (å‚è€ƒå›¾ç‰¹å¾)                     â”‚  â”‚
â”‚  â”‚                             â”‚ Query=UNetç‰¹å¾, Key/Value=å‚è€ƒå›¾ç‰¹å¾     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚    å¸ƒæœ—æ¡¥: x_0 (çº¿ç¨¿é€šé“å¤åˆ¶) â—„â”€â”€â”€â”€â”€â”€ BB æ‰©æ•£ â”€â”€â”€â”€â”€â”€â–º y (ç›®æ ‡å½©å›¾GT)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                              ä¸Šè‰²è¾“å‡º Äˆ [3,H,W]
```

### 2.2 åŒæµæ¡ä»¶æ³¨å…¥ç­–ç•¥ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰

æœ¬æ–¹æ¡ˆçš„æ ¸å¿ƒåˆ›æ–°åœ¨äº**åŒæµæ¡ä»¶æ³¨å…¥**è®¾è®¡ï¼Œé’ˆå¯¹ä¸¤ç§ä¸åŒæ€§è´¨çš„æ¡ä»¶ä¿¡æ¯é‡‡ç”¨ä¸åŒçš„æ³¨å…¥æ–¹å¼ï¼š

| æ¡ä»¶ç±»å‹ | æ³¨å…¥æ–¹å¼ | æ³¨å…¥ä½ç½® | è®¾è®¡ç†ç”± |
|:---|:---|:---|:---|
| **ç»“æ„æ¡ä»¶ï¼ˆçº¿ç¨¿ï¼‰** | Concatæ‹¼æ¥ | UNetè¾“å…¥å±‚ | ç»“æ„ä¿¡æ¯éœ€é€åƒç´ ç²¾ç¡®ä¿æŒï¼ŒConcatæ˜¯æœ€ç›´æ¥æ— æŸçš„æ–¹å¼ |
| **è¯­ä¹‰é¢œè‰²æ¡ä»¶ï¼ˆå‚è€ƒå›¾ï¼‰** | Cross-Attention | UNetä¸­é—´å±‚ | é¢œè‰²è¯­ä¹‰éœ€å…¨å±€å¯¹é½ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯å»ºç«‹è·¨å›¾åƒå¯¹åº” |

**è®¾è®¡ä¼˜åŠ¿**ï¼š
- **æ­£äº¤è§£è€¦**ï¼šç»“æ„ä¸é¢œè‰²æ¡ä»¶çš„åŠŸèƒ½è¾¹ç•Œæ¸…æ™°ï¼Œäº’ä¸å¹²æ‰°
- **å®ç°ç®€æ´**ï¼šæ— éœ€å¤æ‚çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æˆ–å¤šé˜¶æ®µè®­ç»ƒ
- **æ•ˆæœä¿éšœ**ï¼šçº¿ç¨¿ç¡¬æ€§çº¦æŸç»“æ„ï¼Œæ³¨æ„åŠ›æ˜¾å¼å¯¹é½é¢œè‰²

---

## ä¸‰ã€å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹æ•°å­¦åŸç†

### 3.1 å¸ƒæœ—æ¡¥ vs æ ‡å‡†æ‰©æ•£

| ç‰¹æ€§ | æ ‡å‡†DDPM | å¸ƒæœ—æ¡¥BBDM |
|:---|:---|:---|
| èµ·ç‚¹ | çº¯é«˜æ–¯å™ªå£° $\mathcal{N}(0, I)$ | å›ºå®šæ¡ä»¶ $x_0$ |
| ç»ˆç‚¹ | æ— çº¦æŸ | å›ºå®šç›®æ ‡ $y$ |
| é€‚ç”¨åœºæ™¯ | æ— æ¡ä»¶/å¼±æ¡ä»¶ç”Ÿæˆ | æ¡ä»¶å›¾åƒç¿»è¯‘ |

### 3.2 å‰å‘æ‰©æ•£è¿‡ç¨‹

å¸ƒæœ—æ¡¥çš„ä¸­é—´çŠ¶æ€ $x_t$ å®šä¹‰ä¸ºï¼š

$$x_t = (1 - m_t) \cdot x_0 + m_t \cdot y + \sigma_t \cdot \epsilon$$

å…¶ä¸­ï¼š
- $x_0$ï¼šèµ·ç‚¹ï¼Œå³çº¿ç¨¿ $L$ï¼ˆ1é€šé“ï¼‰é€šè¿‡é€šé“å¤åˆ¶ï¼ˆrepeatï¼‰æ“ä½œæ‰©å±•å¾—åˆ°çš„3é€šé“å¼ é‡ $L.\text{repeat}(3) \in \mathbb{R}^{3 \times H \times W}$ï¼Œå…¶RGBä¸‰ä¸ªé€šé“çš„å€¼å®Œå…¨ç›¸åŒ

> **æ³¨æ„**ï¼šæ­¤å¤„çš„ $x_0$ æ˜¯ç”¨äºå¸ƒæœ—æ¡¥æ‰©æ•£è¿‡ç¨‹çš„3é€šé“èµ·ç‚¹ï¼›è€Œä¸ $x_t$ è¿›è¡ŒConcatæ“ä½œè¾“å…¥UNetçš„æ˜¯åŸå§‹çš„1é€šé“çº¿ç¨¿ $L$ï¼ŒäºŒè€…éœ€åŒºåˆ†
- $y$ï¼šç»ˆç‚¹ï¼Œå³ç›®æ ‡å½©è‰²å›¾åƒï¼ˆGround Truthï¼‰
- $m_t = t / T$ï¼šä»èµ·ç‚¹åˆ°ç»ˆç‚¹çš„çº¿æ€§æ’å€¼ç³»æ•°
- $\sigma_t = \sqrt{2s \cdot m_t \cdot (1 - m_t)}$ï¼šæ–¹å·®è°ƒåº¦ï¼Œ$s$ ä¸ºæœ€å¤§æ–¹å·®ç³»æ•°ï¼ˆé»˜è®¤1.0ï¼‰
- $\epsilon \sim \mathcal{N}(0, I)$ï¼šæ ‡å‡†é«˜æ–¯å™ªå£°

**ç«¯ç‚¹çº¦æŸ**ï¼š
- $t=0$ æ—¶ï¼š$m_0=0$ï¼Œ$\sigma_0=0$ï¼Œ$x_0 = \text{gray}(L)$
- $t=T$ æ—¶ï¼š$m_T=1$ï¼Œ$\sigma_T=0$ï¼Œ$x_T = y$

### 3.3 è®­ç»ƒç›®æ ‡

é‡‡ç”¨ `grad` ç›®æ ‡å‡½æ•°ï¼ˆBBDMé»˜è®¤ï¼‰ï¼š

$$\text{objective} = m_t \cdot (y - x_0) + \sigma_t \cdot \epsilon$$

è®­ç»ƒæŸå¤±ï¼š

$$\mathcal{L}_{diff} = \mathbb{E}_{x_0, y, t, \epsilon} \left[ \| \text{objective} - \text{objective}_\theta(\text{concat}(x_t, L), t, \text{Enc}(R)) \|_1 \right]$$

å…¶ä¸­ $\text{objective}_\theta$ æ˜¯UNetçš„é¢„æµ‹è¾“å‡ºï¼Œå…¶å®é™…è¾“å…¥ä¸ºï¼š
- $\text{concat}(x_t, L)$ï¼šå«å™ªçŠ¶æ€ $x_t$ï¼ˆ3é€šé“ï¼‰ä¸çº¿ç¨¿ $L$ï¼ˆ1é€šé“ï¼‰æ‹¼æ¥åçš„4é€šé“å¼ é‡
- $t$ï¼šæ—¶é—´æ­¥åµŒå…¥
- $\text{Enc}(R)$ï¼šå‚è€ƒå›¾ $R$ ç»ç¼–ç å™¨æå–çš„ç‰¹å¾ï¼Œé€šè¿‡Cross-Attentionæ³¨å…¥UNetä¸­é—´å±‚

### 3.4 é€†å‘é‡‡æ ·è¿‡ç¨‹

ä» $t=T$ å¼€å§‹ï¼Œè®¾å®š $x_T = L.\text{repeat}(3)$ï¼ˆæ¨ç†æ—¶èµ·ç‚¹ä¸ºçº¿ç¨¿é€šé“å¤åˆ¶å¾—åˆ°çš„3é€šé“å¼ é‡ï¼‰ï¼Œé€’æ¨è®¡ç®—ï¼š

```
è¾“å…¥: çº¿ç¨¿ L, å‚è€ƒå›¾ R
    â”‚
    â–¼
1. åˆå§‹åŒ–: x_T = L.repeat(3)  # çº¿ç¨¿(1é€šé“)å¤åˆ¶æ‰©å±•ä¸º3é€šé“
    â”‚
    â–¼
2. ç¼–ç å‚è€ƒå›¾: context = ReferenceEncoder(R)
    â”‚
    â–¼
3. è¿­ä»£å»å™ª (for t = T, T-Î”, ..., Î”, 0):
   â”‚
   â”œâ”€ æ‹¼æ¥è¾“å…¥: input = concat(x_t, L)  # [4, H, W]
   â”œâ”€ UNeté¢„æµ‹: objective_pred = UNet(input, t, context)
   â”œâ”€ é‡å»ºx0: xÌ‚_0 = predict_x0_from_objective(x_t, objective_pred, t)
   â””â”€ åéªŒé‡‡æ ·: x_{t-Î”} = posterior_sample(xÌ‚_0, x_t, t)
    â”‚
    â–¼
4. è¾“å‡º: Äˆ = x_0
```

---

## å››ã€å…³é”®æ¨¡å—è¯¦ç»†è®¾è®¡

### 4.1 å‚è€ƒå›¾ç¼–ç å™¨ (Reference Encoder)

é‡‡ç”¨è½»é‡çº§CNNæ¶æ„ï¼Œæå–å‚è€ƒå›¾çš„å¤šå°ºåº¦é¢œè‰²è¯­ä¹‰ç‰¹å¾ï¼š

```python
class ReferenceEncoder(nn.Module):
    """
    è½»é‡çº§å‚è€ƒå›¾ç¼–ç å™¨
    è¾“å…¥: å‚è€ƒå›¾ R [B, 3, H, W]
    è¾“å‡º: ç‰¹å¾åºåˆ— [B, feature_dim, h, w]ï¼Œç”¨äºCross-Attention
    """
    def __init__(self, in_channels=3, feature_dim=256):
        super().__init__()
        self.encoder = nn.Sequential(
            # ç¬¬1å±‚: HÃ—W â†’ H/2Ã—W/2
            nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1),
            nn.GroupNorm(8, 64),
            nn.SiLU(),
            
            # ç¬¬2å±‚: H/2Ã—W/2 â†’ H/4Ã—W/4
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.GroupNorm(8, 128),
            nn.SiLU(),
            
            # ç¬¬3å±‚: H/4Ã—W/4 â†’ H/8Ã—W/8
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.GroupNorm(8, 256),
            nn.SiLU(),
            
            # ç¬¬4å±‚: H/8Ã—W/8 â†’ H/16Ã—W/16
            nn.Conv2d(256, feature_dim, kernel_size=3, stride=2, padding=1),
            nn.GroupNorm(8, feature_dim),
            nn.SiLU(),
        )
        
    def forward(self, ref_image):
        """
        Args:
            ref_image: [B, 3, H, W], èŒƒå›´[-1, 1]
        Returns:
            features: [B, feature_dim, H/16, W/16]
        """
        return self.encoder(ref_image)
```

**è®¾è®¡è¦ç‚¹**ï¼š
- 4å±‚ä¸‹é‡‡æ ·ï¼Œå°†256Ã—256å›¾åƒå‹ç¼©åˆ°16Ã—16ç‰¹å¾å›¾
- ä½¿ç”¨GroupNorm + SiLUæ¿€æ´»ï¼Œè®­ç»ƒæ›´ç¨³å®š
- è¾“å‡ºç»´åº¦256ï¼Œä¸UNetä¸­é—´å±‚é€šé“æ•°åŒ¹é…
- å®Œå…¨å¯è®­ç»ƒï¼Œæ— éœ€é¢„è®­ç»ƒæƒé‡

### 4.2 Cross-Attention æ¨¡å—

åœ¨UNetçš„Middle Blockæ’å…¥äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œå®ç°è¯­ä¹‰é¢œè‰²å¯¹é½ï¼š

```python
class ColorCrossAttention(nn.Module):
    """
    é¢œè‰²äº¤å‰æ³¨æ„åŠ›æ¨¡å—
    Query: UNetä¸­é—´å±‚ç‰¹å¾ (ä»£è¡¨"å“ªé‡Œéœ€è¦é¢œè‰²")
    Key/Value: å‚è€ƒå›¾ç¼–ç ç‰¹å¾ (ä»£è¡¨"é¢œè‰²ä»å“ªæ¥")
    """
    def __init__(self, query_dim, context_dim, heads=8, dim_head=32):
        super().__init__()
        self.heads = heads
        self.dim_head = dim_head
        inner_dim = heads * dim_head
        
        # çº¿æ€§æŠ•å½±
        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)
        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)
        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)
        self.to_out = nn.Linear(inner_dim, query_dim)
        
        # ç¼©æ”¾å› å­
        self.scale = dim_head ** -0.5
        
    def forward(self, x, context):
        """
        Args:
            x: UNetç‰¹å¾ [B, H*W, query_dim]
            context: å‚è€ƒå›¾ç‰¹å¾ [B, h*w, context_dim]
        Returns:
            out: èåˆåç‰¹å¾ [B, H*W, query_dim]
        """
        B, N, _ = x.shape
        
        # è®¡ç®—Q, K, V
        q = self.to_q(x)      # [B, N, inner_dim]
        k = self.to_k(context) # [B, M, inner_dim]
        v = self.to_v(context) # [B, M, inner_dim]
        
        # é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
        q = q.view(B, N, self.heads, self.dim_head).transpose(1, 2)  # [B, heads, N, dim_head]
        k = k.view(B, -1, self.heads, self.dim_head).transpose(1, 2) # [B, heads, M, dim_head]
        v = v.view(B, -1, self.heads, self.dim_head).transpose(1, 2) # [B, heads, M, dim_head]
        
        # æ³¨æ„åŠ›è®¡ç®—
        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # [B, heads, N, M]
        attn = F.softmax(attn, dim=-1)
        
        # åŠ æƒèšåˆ
        out = torch.matmul(attn, v)  # [B, heads, N, dim_head]
        out = out.transpose(1, 2).reshape(B, N, -1)  # [B, N, inner_dim]
        
        return self.to_out(out)
```

**æ³¨æ„åŠ›æœºåˆ¶çš„è¯­ä¹‰å«ä¹‰**ï¼š
- **Query (æ¥è‡ªUNet)**ï¼šç¼–ç äº†çº¿ç¨¿çš„ç©ºé—´ç»“æ„ä¿¡æ¯ï¼Œä»£è¡¨"è¿™ä¸ªä½ç½®éœ€è¦ä»€ä¹ˆé¢œè‰²"
- **Key/Value (æ¥è‡ªå‚è€ƒå›¾)**ï¼šç¼–ç äº†å‚è€ƒå›¾çš„é¢œè‰²åˆ†å¸ƒï¼Œä»£è¡¨"å‚è€ƒå›¾ä¸­æœ‰å“ªäº›é¢œè‰²å¯ç”¨"
- **Attentionæƒé‡**ï¼šè‡ªåŠ¨å­¦ä¹ çº¿ç¨¿åŒºåŸŸä¸å‚è€ƒå›¾é¢œè‰²åŒºåŸŸçš„å¯¹åº”å…³ç³»

### 4.3 UNet ç»“æ„ä¿®æ”¹

åŸºäºåŸå§‹BBDMçš„UNetï¼Œè¿›è¡Œä»¥ä¸‹ä¿®æ”¹ï¼š

```python
class ModifiedUNet(nn.Module):
    """
    ä¿®æ”¹åçš„UNetï¼Œæ”¯æŒåŒæµæ¡ä»¶æ³¨å…¥
    """
    def __init__(self, config):
        super().__init__()
        
        # ä¿®æ”¹1: è¾“å…¥é€šé“ä»3æ”¹ä¸º4 (x_t: 3é€šé“ + L: 1é€šé“)
        self.in_channels = 4
        
        # åŸæœ‰çš„Encoderã€Middleã€Decoderç»“æ„ä¿æŒä¸å˜
        self.encoder = Encoder(in_channels=4, ...)  # æ³¨æ„è¾“å…¥é€šé“
        self.middle = MiddleBlock(...)
        self.decoder = Decoder(...)
        
        # ä¿®æ”¹2: åœ¨Middle Blockåæ·»åŠ Cross-Attention
        self.cross_attn = ColorCrossAttention(
            query_dim=config.model_channels * 4,  # Middleå±‚é€šé“æ•°
            context_dim=256,  # å‚è€ƒå›¾ç¼–ç å™¨è¾“å‡ºç»´åº¦
            heads=8,
            dim_head=32
        )
        
    def forward(self, x_t, t, lineart, ref_context):
        """
        Args:
            x_t: å«å™ªçŠ¶æ€ [B, 3, H, W]
            t: æ—¶é—´æ­¥ [B]
            lineart: çº¿ç¨¿ [B, 1, H, W]
            ref_context: å‚è€ƒå›¾ç‰¹å¾ [B, C, h, w]
        """
        # 1. Concatçº¿ç¨¿åˆ°è¾“å…¥
        x = torch.cat([x_t, lineart], dim=1)  # [B, 4, H, W]
        
        # 2. æ—¶é—´åµŒå…¥
        t_emb = self.time_embed(t)
        
        # 3. Encoder
        h, skips = self.encoder(x, t_emb)
        
        # 4. Middle Block
        h = self.middle(h, t_emb)
        
        # 5. Cross-Attention (å…³é”®!)
        B, C, H, W = h.shape
        h_flat = h.flatten(2).transpose(1, 2)  # [B, H*W, C]
        ctx_flat = ref_context.flatten(2).transpose(1, 2)  # [B, h*w, ctx_dim]
        h_attn = self.cross_attn(h_flat, ctx_flat)  # [B, H*W, C]
        h = h + h_attn.transpose(1, 2).view(B, C, H, W)  # æ®‹å·®è¿æ¥
        
        # 6. Decoder
        out = self.decoder(h, skips, t_emb)
        
        return out
```

---

## äº”ã€æŸå¤±å‡½æ•°è®¾è®¡

### 5.1 æ€»ä½“æŸå¤±å‡½æ•°

$$\mathcal{L}_{total} = \mathcal{L}_{diff} + \lambda_{edge} \cdot \mathcal{L}_{edge} + \lambda_{lpips} \cdot \mathcal{L}_{lpips}$$

**æ¨èæƒé‡**ï¼š$\lambda_{edge} = 1.0$ï¼Œ$\lambda_{lpips} = 0.1$ï¼ˆå¯é€‰ï¼‰

### 5.2 ä¸»æŸå¤±ï¼šå¸ƒæœ—æ¡¥æ‰©æ•£æŸå¤±

$$\mathcal{L}_{diff} = \mathbb{E}_{t, x_0, y, \epsilon} \left[ \| \text{objective} - \text{objective}_\theta \|_1 \right]$$

ä½¿ç”¨L1æŸå¤±ï¼Œå¯¹è¾¹ç¼˜æ›´å‹å¥½ï¼ˆç›¸æ¯”L2æŸå¤±çš„å¹³æ»‘å€¾å‘ï¼‰ã€‚

### 5.3 è¾…åŠ©æŸå¤±ï¼šè¾¹ç¼˜æ„ŸçŸ¥æŸå¤±ï¼ˆåˆ›æ–°ç‚¹ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šåˆ©ç”¨çº¿ç¨¿å›¾åƒä½œä¸ºå¤©ç„¶çš„è¾¹ç¼˜å…ˆéªŒï¼Œåœ¨è¾¹ç¼˜åŒºåŸŸæ–½åŠ æ›´å¼ºçš„çº¦æŸã€‚

```python
class EdgeAwareLoss(nn.Module):
    """
    è¾¹ç¼˜æ„ŸçŸ¥æŸå¤±
    åœ¨çº¿ç¨¿è¾¹ç¼˜åŒºåŸŸï¼ˆæš—éƒ¨ï¼‰ç»™äºˆæ›´é«˜çš„é‡å»ºè¯¯å·®æƒé‡
    """
    def __init__(self, edge_weight=2.0, threshold=0.5):
        super().__init__()
        self.edge_weight = edge_weight
        self.threshold = threshold
        
    def forward(self, pred, target, lineart):
        """
        Args:
            pred: é¢„æµ‹çš„objectiveæˆ–é‡å»ºå›¾åƒ [B, 3, H, W]
            target: ç›®æ ‡objectiveæˆ–GTå›¾åƒ [B, 3, H, W]
            lineart: çº¿ç¨¿å›¾åƒ [B, 1, H, W], èŒƒå›´[-1, 1]
        """
        # 1. ç”Ÿæˆè¾¹ç¼˜æ©ç 
        # çº¿ç¨¿ä¸­æš—çš„éƒ¨åˆ†ï¼ˆçº¿æ¡ï¼‰è§†ä¸ºè¾¹ç¼˜åŒºåŸŸ
        # lineartèŒƒå›´[-1, 1]ï¼Œçº¿æ¡ä¸ºæš—è‰²ï¼ˆæ¥è¿‘-1ï¼‰ï¼Œç©ºç™½ä¸ºäº®è‰²ï¼ˆæ¥è¿‘1ï¼‰
        edge_mask = (lineart < self.threshold).float()  # [B, 1, H, W]
        
        # å¯é€‰: è†¨èƒ€è¾¹ç¼˜åŒºåŸŸï¼Œè¦†ç›–çº¿æ¡é‚»åŸŸ
        edge_mask = self.dilate(edge_mask, kernel_size=3)
        
        # 2. è®¡ç®—åŸºç¡€é‡å»ºæŸå¤±
        base_loss = F.l1_loss(pred, target, reduction='none')  # [B, 3, H, W]
        
        # 3. è¾¹ç¼˜åŒºåŸŸåŠ æƒ
        # è¾¹ç¼˜åŒºåŸŸæƒé‡=edge_weight, éè¾¹ç¼˜åŒºåŸŸæƒé‡=1
        weight_map = 1.0 + (self.edge_weight - 1.0) * edge_mask  # [B, 1, H, W]
        weighted_loss = base_loss * weight_map
        
        return weighted_loss.mean()
    
    def dilate(self, mask, kernel_size=3):
        """å½¢æ€å­¦è†¨èƒ€ï¼Œæ‰©å¤§è¾¹ç¼˜åŒºåŸŸ"""
        padding = kernel_size // 2
        return F.max_pool2d(mask, kernel_size, stride=1, padding=padding)
```

**è®¾è®¡è¯´æ˜**ï¼š
- ç›´æ¥åˆ©ç”¨è¾“å…¥çº¿ç¨¿ä½œä¸ºè¾¹ç¼˜å…ˆéªŒï¼Œæ— éœ€é¢å¤–è®¡ç®—è¾¹ç¼˜æ£€æµ‹
- è¾¹ç¼˜åŒºåŸŸï¼ˆçº¿ç¨¿æš—éƒ¨ï¼‰æƒé‡è®¾ä¸º2å€ï¼Œå¼ºåˆ¶æ¨¡å‹åœ¨è¾¹ç¼˜å¤„æ›´ç²¾ç¡®
- è†¨èƒ€æ“ä½œè¦†ç›–çº¿æ¡é‚»åŸŸï¼Œé˜²æ­¢è¾¹ç¼˜æº¢è‰²

> **å®ç°å¤‡æ³¨**ï¼šæœ¬æ–¹æ¡ˆä¸­è¾¹ç¼˜æ„ŸçŸ¥æŸå¤±ä½œç”¨äºé¢„æµ‹çš„æ‰©æ•£ç›®æ ‡ $\text{objective}$ ä¸Šã€‚å¦ä¸€ç§å¯é€‰æ–¹æ¡ˆæ˜¯ä½œç”¨äºé‡å»ºçš„å›¾åƒ $\hat{x}_0$ ä¸Šï¼Œåè€…å¯¹è¾¹ç¼˜åŒºåŸŸçš„åƒç´ å€¼çº¦æŸæ›´ç›´æ¥ï¼Œä½†éœ€åœ¨è®­ç»ƒå¾ªç¯ä¸­é¢å¤–æ‰§è¡Œä¸€æ¬¡ $x_0$ é‡å»ºè®¡ç®—ï¼Œå¼€é”€ç•¥å¢ã€‚ä¸¤ç§æ–¹å¼å‡å¯è¡Œï¼Œå¯æ ¹æ®å®éªŒæ•ˆæœé€‰æ‹©ã€‚

### 5.4 å¯é€‰æŸå¤±ï¼šæ„ŸçŸ¥æŸå¤±

```python
class LPIPSLoss(nn.Module):
    """æ„ŸçŸ¥æŸå¤±ï¼Œæå‡ç”Ÿæˆå›¾åƒçš„è§†è§‰è´¨é‡"""
    def __init__(self):
        super().__init__()
        self.lpips = lpips.LPIPS(net='vgg')  # ä½¿ç”¨é¢„è®­ç»ƒVGG
        self.lpips.eval()
        for param in self.lpips.parameters():
            param.requires_grad = False
            
    def forward(self, pred, target):
        return self.lpips(pred, target).mean()
```

---

## å…­ã€æ•°æ®é›†æ„å»º

### 6.1 æ•°æ®ç»“æ„

æ¯ä¸ªè®­ç»ƒæ ·æœ¬ä¸ºä¸‰å…ƒç»„ $(L, R, C)$ï¼š
- $L$ï¼šçº¿ç¨¿å›¾åƒï¼ˆç°åº¦ï¼‰
- $R$ï¼šå‚è€ƒå›¾åƒï¼ˆå½©è‰²ï¼‰
- $C$ï¼šç›®æ ‡å½©è‰²å›¾åƒï¼ˆGround Truthï¼‰

### 6.2 æ•°æ®é›†ç±»å®ç°

```python
@Registers.dataset.register_with_name('anime_colorization')
class AnimeColorizationDataset(Dataset):
    """
    åŠ¨æ¼«çº¿ç¨¿ä¸Šè‰²æ•°æ®é›†
    """
    def __init__(self, dataset_config, stage='train'):
        self.stage = stage
        self.image_size = dataset_config.get('image_size', 256)
        self.data_root = dataset_config.get('data_root')
        
        # åŠ è½½å›¾åƒè·¯å¾„åˆ—è¡¨
        self.image_paths = self._load_paths()
        
        # æ•°æ®å¢å¼º
        self.use_tps = dataset_config.get('use_tps', True) and stage == 'train'
        self.tps_scale = dataset_config.get('tps_scale', 0.2)
        
    def __getitem__(self, index):
        # 1. åŠ è½½å½©è‰²å›¾åƒ (Ground Truth)
        img_path = self.image_paths[index]
        color_img = Image.open(img_path).convert('RGB')
        color_img = self.resize_and_crop(color_img)
        
        # 2. æå–çº¿ç¨¿
        lineart = self.extract_lineart(color_img)
        
        # 3. ç”Ÿæˆå‚è€ƒå›¾
        if self.stage == 'train':
            # è®­ç»ƒæ—¶: å¯¹å½©å›¾æ–½åŠ TPSå˜å½¢ä½œä¸ºå‚è€ƒå›¾
            ref_img = self.apply_tps_augment(color_img) if self.use_tps else color_img
            # é¢å¤–çš„é¢œè‰²æŠ–åŠ¨
            ref_img = self.color_jitter(ref_img)
        else:
            # æµ‹è¯•æ—¶: ä½¿ç”¨åŒä¸€å¼ å›¾ï¼ˆæˆ–æŒ‡å®šçš„å‚è€ƒå›¾ï¼‰
            ref_img = color_img
        
        # 4. è½¬æ¢ä¸ºTensorå¹¶å½’ä¸€åŒ–åˆ°[-1, 1]
        color_tensor = self.to_tensor(color_img)      # [3, H, W]
        lineart_tensor = self.to_tensor(lineart)       # [1, H, W]
        ref_tensor = self.to_tensor(ref_img)           # [3, H, W]
        
        return {
            'GT': color_tensor,        # ç›®æ ‡å½©å›¾ (y in BBDM)
            'lineart': lineart_tensor, # çº¿ç¨¿ (ä½œä¸ºæ¡ä»¶)
            'ref': ref_tensor,         # å‚è€ƒå›¾ (ä½œä¸ºæ¡ä»¶)
            'cond': lineart_tensor,    # èµ·ç‚¹æ¡ä»¶ (x_0 in BBDM, ç”¨gray(L))
        }
    
    def extract_lineart(self, img):
        """
        ä»å½©å›¾æå–çº¿ç¨¿
        å¯é€‰æ–¹æ³•: XDoG, Canny, é¢„è®­ç»ƒçº¿ç¨¿æå–ç½‘ç»œ
        """
        # ç®€åŒ–ç‰ˆ: ä½¿ç”¨è¾¹ç¼˜æ£€æµ‹ + åè‰²
        gray = img.convert('L')
        edges = gray.filter(ImageFilter.FIND_EDGES)
        # åè‰²ä½¿çº¿æ¡ä¸ºé»‘è‰²
        lineart = ImageOps.invert(edges)
        return lineart
    
    def apply_tps_augment(self, img):
        """
        TPS (Thin Plate Spline) å˜å½¢
        å¼ºè¿«æ¨¡å‹å­¦ä¹ è¯­ä¹‰å¯¹åº”è€Œéåƒç´ å¤åˆ¶
        """
        # ä½¿ç”¨korniaæˆ–è‡ªå®šä¹‰å®ç°
        import kornia.augmentation as K
        tps = K.RandomThinPlateSpline(scale=self.tps_scale, p=1.0)
        img_tensor = transforms.ToTensor()(img).unsqueeze(0)
        warped = tps(img_tensor)
        return transforms.ToPILImage()(warped.squeeze(0))
```

### 6.3 TPSå˜å½¢æ•°æ®å¢å¼º

TPSå˜å½¢æ˜¯è®­ç»ƒç¨³å®šæ€§çš„å…³é”®ï¼Œå¼ºåˆ¶æ¨¡å‹å­¦ä¹ è¯­ä¹‰å¯¹åº”è€Œéæ­»è®°åƒç´ ä½ç½®ï¼š


---

## ä¸ƒã€è®­ç»ƒæµç¨‹

### 7.0 è¶…å‚æ•°æ±‡æ€»

#### æ ¸å¿ƒè¶…å‚æ•°è¡¨

| ç±»åˆ« | å‚æ•° | æ¨èå€¼ | å–å€¼èŒƒå›´ | è¯´æ˜ |
|:---|:---|:---:|:---|:---|
| **æ‰©æ•£è¿‡ç¨‹** | num_timesteps (T) | 1000 | - | å¸ƒæœ—æ¡¥æ ‡å‡†è®¾ç½® |
| | sample_step | 200 | [100, 500] | æ¨ç†æ­¥æ•°ï¼Œè¶Šå¤šè´¨é‡è¶Šé«˜ |
| | max_var (s) | 1.0 | [0.5, 2.0] | æ–¹å·®ç³»æ•°ï¼Œæ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§ |
| | eta | 1.0 | [0, 1] | DDIMéšæœºæ€§ï¼Œ1.0=å®Œå…¨éšæœº |
| | objective | 'grad' | grad/noise | é¢„æµ‹ç›®æ ‡ï¼Œæ¨ègrad |
| **ä¼˜åŒ–å™¨** | optimizer | AdamW | - | æ ‡å‡†é€‰æ‹© |
| | learning_rate | 1e-4 | [5e-5, 2e-4] | æ‰©æ•£æ¨¡å‹æ ‡å‡†å­¦ä¹ ç‡ |
| | weight_decay | 0.01 | [0, 0.05] | L2æ­£åˆ™åŒ– |
| | betas | (0.9, 0.999) | - | AdamåŠ¨é‡å‚æ•° |
| **è®­ç»ƒ** | batch_size | 8 | [4, 32] | å—æ˜¾å­˜é™åˆ¶ |
| | n_epochs | 100 | [50, 200] | è§†æ•°æ®é›†è§„æ¨¡è°ƒæ•´ |
| | grad_clip | 1.0 | [0.5, 2.0] | æ¢¯åº¦è£å‰ªé˜ˆå€¼ |
| **æŸå¤±æƒé‡** | Î»_edge | 1.0 | [0.5, 2.0] | è¾¹ç¼˜æ„ŸçŸ¥æŸå¤±æƒé‡ |
| | Î»_lpips | 0.1 | [0, 0.5] | æ„ŸçŸ¥æŸå¤±æƒé‡ï¼ˆå¯é€‰ï¼‰ |
| **æ•°æ®å¢å¼º** | tps_scale | 0.2 | [0.1, 0.3] | TPSå˜å½¢å¼ºåº¦ |
| **EMA** | ema_rate | 0.9999 | [0.999, 0.9999] | æŒ‡æ•°å¹³æ»‘ç³»æ•° |
| | ema_start | 5000æ­¥ | - | EMAå¯åŠ¨æ—¶æœº |


> **è°ƒå‚ä¼˜å…ˆçº§**ï¼šè¾¹ç¼˜æŸå¤±æƒé‡ > TPSå¼ºåº¦ > æ‰¹é‡å¤§å° > å…¶ä»–ï¼ˆä¿æŒé»˜è®¤ï¼‰

### 7.1 è®­ç»ƒé…ç½®

```yaml
# configs/DualStream-BBDM-Colorization.yaml

runner: "BBDMRunner"

model:
  model_type: "BBDM"
  model_name: "DualStreamBBDM"
  
  # å¸ƒæœ—æ¡¥å‚æ•°
  BB:
    params:
      mt_type: 'linear'           # m_t è°ƒåº¦æ–¹å¼
      objective: 'grad'           # é¢„æµ‹ç›®æ ‡
      loss_type: 'l1'             # L1æŸå¤±å¯¹è¾¹ç¼˜å‹å¥½
      num_timesteps: 1000         # è®­ç»ƒæ­¥æ•° T
      sample_step: 200            # æ¨ç†æ­¥æ•°
      max_var: 1.0                # æœ€å¤§æ–¹å·®ç³»æ•° s
      eta: 1.0                    # DDIMé‡‡æ ·éšæœºæ€§
      skip_sample: True           # å¯ç”¨è·³æ­¥é‡‡æ ·
      sample_type: 'linear'       # å‡åŒ€è·³æ­¥
  
  # UNetå‚æ•°
  UNetParams:
    image_size: 256
    in_channels: 4               # ä¿®æ”¹: 3(x_t) + 1(çº¿ç¨¿)
    out_channels: 3
    model_channels: 128
    channel_mult: [1, 2, 4, 8]   # é€šé“å€å¢
    num_res_blocks: 2
    attention_resolutions: [32, 16, 8]
    dropout: 0.0
    
    # Cross-Attentioné…ç½®
    use_spatial_transformer: True
    transformer_depth: 1
    context_dim: 256             # å‚è€ƒå›¾ç¼–ç å™¨è¾“å‡ºç»´åº¦
  
  # å‚è€ƒå›¾ç¼–ç å™¨å‚æ•°
  RefEncoderParams:
    in_channels: 3
    feature_dim: 256
    
  # æŸå¤±æƒé‡
  loss_weights:
    edge_weight: 1.0             # è¾¹ç¼˜æ„ŸçŸ¥æŸå¤±æƒé‡
    lpips_weight: 0.1            # æ„ŸçŸ¥æŸå¤±æƒé‡ (å¯é€‰)

# æ•°æ®é…ç½®
data:
  dataset_type: 'anime_colorization'
  dataset_config:
    data_root: './data/anime_colorization'
    image_size: 256
    use_tps: True
    tps_scale: 0.2
  train:
    batch_size: 8
    shuffle: True
  val:
    batch_size: 4
    shuffle: False

# è®­ç»ƒé…ç½®
training:
  n_epochs: 100
  save_interval: 10
  sample_interval: 5
  
  optimizer:
    type: 'AdamW'
    lr: 1.0e-4
    weight_decay: 0.01
    betas: [0.9, 0.999]
  
  scheduler:
    type: 'CosineAnnealingLR'
    T_max: 100
    eta_min: 1.0e-6
  
  # è®­ç»ƒç¨³å®šæ€§
  grad_clip: 1.0                 # æ¢¯åº¦è£å‰ª
  ema_rate: 0.9999               # EMAè¡°å‡ç‡
  ema_start_step: 5000           # EMAå¯åŠ¨æ­¥æ•°
```

### 7.2 è®­ç»ƒä¼ªä»£ç 

```python
def train_epoch(model, dataloader, optimizer, scheduler):
    model.train()
    
    for batch in dataloader:
        gt = batch['GT']           # [B, 3, H, W] ç›®æ ‡å½©å›¾
        lineart = batch['lineart'] # [B, 1, H, W] çº¿ç¨¿
        ref = batch['ref']         # [B, 3, H, W] å‚è€ƒå›¾
        
        # 1. æ„å»ºå¸ƒæœ—æ¡¥èµ·ç‚¹ x_0 = gray(lineart)
        x_0 = lineart.repeat(1, 3, 1, 1)  # [B, 3, H, W]
        y = gt  # ç»ˆç‚¹
        
        # 2. ç¼–ç å‚è€ƒå›¾
        ref_context = model.ref_encoder(ref)  # [B, 256, 16, 16]
        
        # 3. é‡‡æ ·æ—¶é—´æ­¥
        t = torch.randint(0, num_timesteps, (B,), device=device)
        
        # 4. å‰å‘æ‰©æ•£ï¼Œå¾—åˆ° x_t
        noise = torch.randn_like(x_0)
        x_t = q_sample(x_0, y, t, noise)  # å¸ƒæœ—æ¡¥å‰å‘
        
        # 5. è®¡ç®—ç›®æ ‡ objective
        objective = compute_objective(x_0, y, t, noise)  # gradç›®æ ‡
        
        # 6. UNeté¢„æµ‹
        objective_pred = model.unet(x_t, t, lineart, ref_context)
        
        # 7. è®¡ç®—æŸå¤±
        loss_diff = F.l1_loss(objective_pred, objective)
        loss_edge = edge_aware_loss(objective_pred, objective, lineart)
        loss_total = loss_diff + lambda_edge * loss_edge
        
        # 8. åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss_total.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
    scheduler.step()
```

### 7.3 è®­ç»ƒç­–ç•¥

| ç­–ç•¥ | è®¾ç½® | ä½œç”¨ |
|:---|:---|:---|
| ä¼˜åŒ–å™¨ | AdamW, lr=1e-4, weight_decay=0.01 | ç¨³å®šè®­ç»ƒ |
| å­¦ä¹ ç‡è°ƒåº¦ | CosineAnnealingLR | å¹³æ»‘æ”¶æ•› |
| æ¢¯åº¦è£å‰ª | max_norm=1.0 | é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ |
| EMA | rate=0.9999, start=5000æ­¥ | ç¨³å®šç”Ÿæˆè´¨é‡ |
| æ•°æ®å¢å¼º | TPSå˜å½¢ (scale=0.2) | å¢å¼ºæ³›åŒ–æ€§ |

> **å®ç°æ³¨æ„ï¼ˆEMAï¼‰**ï¼šæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰å¯å¹³æ»‘è®­ç»ƒè¿‡ç¨‹ï¼Œè·å¾—æ›´ç¨³å®šçš„æµ‹è¯•æƒé‡ï¼Œé€šå¸¸èƒ½æå‡æœ€ç»ˆæŒ‡æ ‡ã€‚å¯¹äºåˆæ¬¡å®ç°ï¼Œè‹¥ä¸ºç®€åŒ–ä»£ç æµç¨‹å¯æš‚ä¸å¯ç”¨ï¼›ä½†åœ¨æ­£å¼å®éªŒæ—¶æ¨èåŠ å…¥ï¼Œå¹¶å¯¹æ¯”EMAæƒé‡ä¸åŸå§‹æƒé‡çš„ç”Ÿæˆæ•ˆæœå·®å¼‚ã€‚

### 7.4 æ—©åœä¸éªŒè¯ç›‘æ§

#### æ—©åœæœºåˆ¶ (Early Stopping)

æ‰©æ•£æ¨¡å‹å¯èƒ½åœ¨100ä¸ªepochä¹‹å‰å°±å·²æ”¶æ•›ï¼Œä¸ºé¿å…ä¸å¿…è¦çš„è®­ç»ƒå¼€é”€å’Œè¿‡æ‹Ÿåˆé£é™©ï¼Œå»ºè®®å¯ç”¨æ—©åœæœºåˆ¶ï¼š

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|:---|:---:|:---|
| patience | 10 | éªŒè¯æŸå¤±è¿ç»­æ— æ”¹å–„çš„å®¹å¿è½®æ•° |
| min_delta | 1e-4 | åˆ¤å®šä¸º"æœ‰æ”¹å–„"çš„æœ€å°é˜ˆå€¼ |
| monitor | val_loss | ç›‘æ§çš„éªŒè¯æŒ‡æ ‡ |

**æ—©åœé€»è¾‘**ï¼š
1. æ¯ä¸ªepochç»“æŸååœ¨éªŒè¯é›†ä¸Šè®¡ç®—æŸå¤±
2. è‹¥éªŒè¯æŸå¤±æ¯”å†å²æœ€ä¼˜ä¸‹é™è¶…è¿‡`min_delta`ï¼Œåˆ™é‡ç½®è®¡æ•°å™¨å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
3. è‹¥è¿ç»­`patience`ä¸ªepochæ— æ”¹å–„ï¼Œåˆ™åœæ­¢è®­ç»ƒ
4. è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹ç”¨äºæµ‹è¯•

#### éªŒè¯é›†ç›‘æ§

**ç›‘æ§æŒ‡æ ‡é€‰æ‹©**ï¼š

| æŒ‡æ ‡ | é€‚ç”¨åœºæ™¯ | è®¡ç®—å¼€é”€ |
|:---|:---|:---:|
| **val_loss** (æ¨è) | é»˜è®¤é€‰æ‹©ï¼Œç›´æ¥åæ˜ æ¨¡å‹å­¦ä¹ çŠ¶æ€ | ä½ |
| val_lpips | å…³æ³¨æ„ŸçŸ¥è´¨é‡æ—¶ä½¿ç”¨ | ä¸­ |
| val_fid | å…³æ³¨åˆ†å¸ƒè´¨é‡ï¼Œé€šå¸¸ç”¨äºæœ€ç»ˆè¯„ä¼° | é«˜ |

**éªŒè¯ç­–ç•¥å»ºè®®**ï¼š
- **éªŒè¯é¢‘ç‡**ï¼šæ¯ä¸ªepochéªŒè¯ä¸€æ¬¡ï¼Œè¿‡äºé¢‘ç¹ä¼šå¢åŠ è®­ç»ƒæ—¶é—´
- **æ—¶é—´æ­¥é‡‡æ ·**ï¼šéªŒè¯æ—¶å¯¹æ¯ä¸ªæ ·æœ¬éšæœºé‡‡æ ·å¤šä¸ªæ—¶é—´æ­¥ï¼ˆå¦‚5ä¸ªï¼‰å–å¹³å‡ï¼Œä½¿æŸå¤±ä¼°è®¡æ›´ç¨³å®š
- **ä¿å­˜ç­–ç•¥**ï¼šåŒæ—¶ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆval_lossæœ€ä½ï¼‰å’Œå®šæœŸæ£€æŸ¥ç‚¹ï¼Œé˜²æ­¢æ„å¤–ä¸­æ–­

#### YAMLé…ç½®è¡¥å……

```yaml
training:
  # æ—©åœé…ç½®
  early_stopping:
    enabled: True
    patience: 10
    min_delta: 1.0e-4
    monitor: 'val_loss'
    
  # éªŒè¯é…ç½®  
  validation:
    interval: 1          # æ¯ä¸ªepochéªŒè¯ä¸€æ¬¡
    num_t_samples: 5     # éªŒè¯æ—¶é‡‡æ ·çš„æ—¶é—´æ­¥æ•°é‡
```
ã€‚

---

## å…«ã€æ¨ç†æµç¨‹

### 8.1 æ¨ç†ä¼ªä»£ç 

```python
@torch.no_grad()
def inference(model, lineart, ref, num_steps=200):
    """
    æ¨ç†å‡½æ•°
    Args:
        lineart: è¾“å…¥çº¿ç¨¿ [1, 1, H, W]
        ref: å‚è€ƒå›¾ [1, 3, H, W]
        num_steps: é‡‡æ ·æ­¥æ•°
    Returns:
        colorized: ä¸Šè‰²ç»“æœ [1, 3, H, W]
    """
    model.eval()
    
    # 1. åˆå§‹åŒ– x_T = gray(lineart)
    x_t = lineart.repeat(1, 3, 1, 1)  # [1, 3, H, W]
    
    # 2. ç¼–ç å‚è€ƒå›¾
    ref_context = model.ref_encoder(ref)  # [1, 256, 16, 16]
    
    # 3. è®¾ç½®é‡‡æ ·æ—¶é—´æ­¥ (è·³æ­¥é‡‡æ ·)
    timesteps = get_skip_timesteps(num_timesteps=1000, sample_steps=num_steps)
    
    # 4. é€†å‘é‡‡æ ·
    for i, t in enumerate(reversed(timesteps)):
        t_batch = torch.tensor([t], device=device)
        
        # UNeté¢„æµ‹
        objective_pred = model.unet(x_t, t_batch, lineart, ref_context)
        
        # é‡å»º x_0
        x_0_pred = predict_x0_from_objective(x_t, objective_pred, t)
        
        # åéªŒé‡‡æ ·å¾—åˆ° x_{t-1}
        if i < len(timesteps) - 1:
            t_prev = timesteps[-(i+2)]
            x_t = posterior_sample(x_0_pred, x_t, t, t_prev)
        else:
            x_t = x_0_pred
    
    return x_t  # ä¸Šè‰²ç»“æœ
```

### 8.2 æ¨ç†åŠ é€Ÿ

| ç­–ç•¥ | è®¾ç½® | åŠ é€Ÿæ¯” |
|:---|:---|:---|
| è·³æ­¥é‡‡æ · | 1000æ­¥ â†’ 200æ­¥ | 5Ã— |
| çº¿æ€§è·³æ­¥ | sample_type='linear' | - |
| åŠç²¾åº¦æ¨ç† | torch.float16 | ~2Ã— |

---

## ä¹ã€å®éªŒè®¾è®¡

### 9.1 æ¶ˆèå®éªŒ

| å®éªŒç¼–å· | é…ç½® | éªŒè¯ç›®æ ‡ |
|:---:|:---|:---|
| A0 | åŸå§‹BBDMï¼ˆçº¿ç¨¿Concatï¼Œæ— å‚è€ƒå›¾ï¼Œæ— è¾¹ç¼˜æŸå¤±ï¼‰ | åŸºå‡†å¯¹ç…§ï¼ŒéªŒè¯æ‰€æœ‰æ”¹è¿›çš„å¿…è¦æ€§ |
| A1 | A0 + å‚è€ƒå›¾Cross-Attention | å‚è€ƒå›¾æ³¨å…¥çš„æœ‰æ•ˆæ€§ |
| A2 | A1 + è¾¹ç¼˜æ„ŸçŸ¥æŸå¤± | è¾¹ç¼˜æŸå¤±çš„æœ‰æ•ˆæ€§ |
| A3 | A2 + TPSå¢å¼º | æ•°æ®å¢å¼ºçš„ä½œç”¨ï¼ˆå®Œæ•´æ–¹æ¡ˆï¼‰ |
| A4 | A3ï¼Œä½†å‚è€ƒå›¾æ”¹ç”¨Concatæ³¨å…¥ | Cross-Attention vs Concatæ³¨å…¥æ–¹å¼å¯¹æ¯” |

**æ¶ˆèé€»è¾‘è¯´æ˜**ï¼šä»A0ï¼ˆåŸå§‹BBDMï¼‰å¼€å§‹ï¼Œé€æ­¥å åŠ å„åˆ›æ–°æ¨¡å—ï¼ˆA0â†’A1â†’A2â†’A3ï¼‰ï¼Œå½¢æˆå®Œæ•´çš„æ¶ˆèé“¾ã€‚æ¯ä¸€æ­¥çš„å¢ç›Šå¯ç‹¬ç«‹é‡åŒ–ï¼Œä»è€Œæœ‰åŠ›è¯æ˜å„æ¨¡å—çš„ç‹¬ç«‹è´¡çŒ®ã€‚A4ç”¨äºéªŒè¯å‚è€ƒå›¾æ³¨å…¥æ–¹å¼çš„é€‰æ‹©ã€‚

### 9.2 è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | è¯„ä¼°ç»´åº¦ | ç›®æ ‡ |
|:---|:---|:---|
| **FID â†“** | ç”Ÿæˆè´¨é‡åˆ†å¸ƒ | < 30 |
| **LPIPS â†“** | æ„ŸçŸ¥ç›¸ä¼¼åº¦ | < 0.15 |
| **PSNR â†‘** | åƒç´ ç²¾åº¦ | > 25 dB |
| **SSIM â†‘** | ç»“æ„ç›¸ä¼¼åº¦ | > 0.85 |
| **Edge IoU â†‘** | è¾¹ç¼˜ä¿æŒåº¦ | > 0.90 |
| **Color Accuracy** | é¢œè‰²è¿ç§»å‡†ç¡®æ€§ | ä¸»è§‚è¯„ä¼° |

### 9.3 å¯¹æ¯”æ–¹æ³•

| æ–¹æ³• | ç±»å‹ | å¯¹æ¯”æ„ä¹‰ |
|:---|:---|:---|
| Pix2Pix | GAN | ç»å…¸baseline |
| Reference-based Sketch Colorization | GAN+å‚è€ƒ | åŒä»»åŠ¡å¯¹æ¯” |
| ControlNet (SD) | æ½œåœ¨ç©ºé—´æ‰©æ•£ | æ‰©æ•£æ¨¡å‹å¯¹æ¯” |
| BBDM (åŸå§‹) | åƒç´ åŸŸæ‰©æ•£ | æ¶ˆèå¯¹æ¯” |

---

## åã€åˆ›æ–°ç‚¹æ€»ç»“

### 10.1 è®ºæ–‡è´¡çŒ®

1. **åŒæµæ¡ä»¶æ³¨å…¥æ¡†æ¶**ï¼šæå‡ºäº†ä¸€ç§é’ˆå¯¹å‚è€ƒå›¾å¼•å¯¼çº¿ç¨¿ä¸Šè‰²ä»»åŠ¡çš„åŒæµæ¡ä»¶æ³¨å…¥æ¡†æ¶ï¼Œé€šè¿‡è§£è€¦çš„ç»“æ„æµä¸è¯­ä¹‰æµï¼Œåˆ†åˆ«é«˜æ•ˆå¤„ç†å‡ ä½•çº¦æŸä¸é¢œè‰²è¿ç§»ï¼š
   - ç»“æ„æµï¼ˆConcatï¼‰ï¼šçº¿ç¨¿ç›´æ¥æ‹¼æ¥è‡³UNetè¾“å…¥ï¼Œé€åƒç´ ä¿æŒç»“æ„ä¿¡æ¯
   - è¯­ä¹‰æµï¼ˆCross-Attentionï¼‰ï¼šå‚è€ƒå›¾ç‰¹å¾é€šè¿‡äº¤å‰æ³¨æ„åŠ›æ³¨å…¥ï¼Œå®ç°è·¨å›¾åƒçš„é¢œè‰²è¯­ä¹‰å¯¹é½

2. **è¾¹ç¼˜æ„ŸçŸ¥æŸå¤±å‡½æ•°**ï¼šåˆ©ç”¨çº¿ç¨¿ä½œä¸ºå¤©ç„¶è¾¹ç¼˜å…ˆéªŒï¼Œåœ¨è¾¹ç¼˜åŒºåŸŸæ–½åŠ åŠ æƒçº¦æŸï¼Œæœ‰æ•ˆç¼“è§£æ‰©æ•£æ¨¡å‹çš„è¾¹ç¼˜æ¨¡ç³Šé—®é¢˜

3. **è½»é‡çº§å‚è€ƒå›¾ç¼–ç å™¨**ï¼šè®¾è®¡4å±‚CNNç¼–ç å™¨æ›¿ä»£å¤æ‚çš„CLIP/ViTï¼Œå®ç°é«˜æ•ˆçš„é¢œè‰²ç‰¹å¾æå–ï¼Œæ˜“äºç«¯åˆ°ç«¯è®­ç»ƒ

### 10.2 æŠ€æœ¯è·¯çº¿å›¾

```
                    å‚è€ƒå›¾å¼•å¯¼çº¿ç¨¿ä¸Šè‰²
                          â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                â–¼                â–¼
    ç»“æ„ä¿æŒ          è¯­ä¹‰å¯¹é½          è¾¹ç¼˜é”åˆ©
         â”‚                â”‚                â”‚
         â–¼                â–¼                â–¼
  çº¿ç¨¿Concatæ³¨å…¥    Cross-Attention    è¾¹ç¼˜æ„ŸçŸ¥æŸå¤±
         â”‚                â”‚                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
              åƒç´ åŸŸå¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹
                          â”‚
                          â–¼
                 é«˜è´¨é‡ä¸Šè‰²ç»“æœ
```

---

## åä¸€ã€é¢„æœŸå·¥ä½œé‡è¯„ä¼°

| ä»»åŠ¡ | å·¥ä½œé‡ | é¢„è®¡æ—¶é—´ |
|:---|:---:|:---:|
| æ•°æ®é›†æ„å»ºä¸é¢„å¤„ç† | ğŸŸ¡ ä¸­ç­‰ | 1å‘¨ |
| å‚è€ƒå›¾ç¼–ç å™¨å®ç° | ğŸŸ¢ ç®€å• | 2å¤© |
| UNetä¿®æ”¹ (Concat + Cross-Attention) | ğŸŸ¡ ä¸­ç­‰ | 3å¤© |
| è¾¹ç¼˜æ„ŸçŸ¥æŸå¤±å®ç° | ğŸŸ¢ ç®€å• | 1å¤© |
| è®­ç»ƒä»£ç æ•´åˆ | ğŸŸ¡ ä¸­ç­‰ | 3å¤© |
| æ¨¡å‹è®­ç»ƒä¸è°ƒå‚ | ğŸŸ¡ ä¸­ç­‰ | 2å‘¨ |
| å®éªŒä¸æ¶ˆèåˆ†æ | ğŸŸ¡ ä¸­ç­‰ | 1å‘¨ |
| **æ€»è®¡** | - | **~5å‘¨** |

---

## åäºŒã€å‚è€ƒæ–‡çŒ®

1. Li, B., et al. "BBDM: Image-to-Image Translation with Brownian Bridge Diffusion Models." CVPR 2023.
2. Zhang, L., et al. "Adding Conditional Control to Text-to-Image Diffusion Models." ICCV 2023. (ControlNet)
3. Rombach, R., et al. "High-Resolution Image Synthesis with Latent Diffusion Models." CVPR 2022. (Stable Diffusion)
4. Isola, P., et al. "Image-to-Image Translation with Conditional Adversarial Networks." CVPR 2017. (Pix2Pix)

---

*æ–‡æ¡£ç‰ˆæœ¬: v1.0*  
*æœ€åæ›´æ–°: 2026å¹´1æœˆ23æ—¥*
